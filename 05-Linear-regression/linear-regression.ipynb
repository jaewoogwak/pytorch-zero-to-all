{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from torch import tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = tensor([[1.0], [2.0], [3.0]])\n",
    "y_data = tensor([[2.0], [4.0], [6.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear module\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        self.linear = torch.nn.Linear(1, 1) # One in (x_data) and on out (y_data)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Variable of input data ad we must return a Variable of output data.\n",
    "        We can use Modules defined in the constructor as well as arbitrary operators on Variables.\n",
    "        \"\"\"\n",
    "        \n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our moel\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "criterion = torch.nn.MSELoss(reduction=\"sum\") # output이 loss값의 합이 됨\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 6.059188422113948e-07 \n",
      "Epoch: 1 | Loss: 5.974845862510847e-07 \n",
      "Epoch: 2 | Loss: 5.884847382731095e-07 \n",
      "Epoch: 3 | Loss: 5.801732640975388e-07 \n",
      "Epoch: 4 | Loss: 5.719643922930118e-07 \n",
      "Epoch: 5 | Loss: 5.634222475237038e-07 \n",
      "Epoch: 6 | Loss: 5.55548979264131e-07 \n",
      "Epoch: 7 | Loss: 5.475170041790989e-07 \n",
      "Epoch: 8 | Loss: 5.395008884079289e-07 \n",
      "Epoch: 9 | Loss: 5.318408966559218e-07 \n",
      "Epoch: 10 | Loss: 5.242773681857216e-07 \n",
      "Epoch: 11 | Loss: 5.164340564078884e-07 \n",
      "Epoch: 12 | Loss: 5.08981258917629e-07 \n",
      "Epoch: 13 | Loss: 5.018710567128437e-07 \n",
      "Epoch: 14 | Loss: 4.948108767166559e-07 \n",
      "Epoch: 15 | Loss: 4.875983563579211e-07 \n",
      "Epoch: 16 | Loss: 4.80639585020981e-07 \n",
      "Epoch: 17 | Loss: 4.737308358926384e-07 \n",
      "Epoch: 18 | Loss: 4.6687210897289333e-07 \n",
      "Epoch: 19 | Loss: 4.6006340426174575e-07 \n",
      "Epoch: 20 | Loss: 4.5357887756836135e-07 \n",
      "Epoch: 21 | Loss: 4.471789907256607e-07 \n",
      "Epoch: 22 | Loss: 4.404776632327412e-07 \n",
      "Epoch: 23 | Loss: 4.341331987234298e-07 \n",
      "Epoch: 24 | Loss: 4.281387759874633e-07 \n",
      "Epoch: 25 | Loss: 4.219975835439982e-07 \n",
      "Epoch: 26 | Loss: 4.157880653110624e-07 \n",
      "Epoch: 27 | Loss: 4.09922222388559e-07 \n",
      "Epoch: 28 | Loss: 4.0376585275225807e-07 \n",
      "Epoch: 29 | Loss: 3.979856160185591e-07 \n",
      "Epoch: 30 | Loss: 3.923203735212155e-07 \n",
      "Epoch: 31 | Loss: 3.8687613823640277e-07 \n",
      "Epoch: 32 | Loss: 3.811829287769797e-07 \n",
      "Epoch: 33 | Loss: 3.758169100365194e-07 \n",
      "Epoch: 34 | Loss: 3.7031219335403875e-07 \n",
      "Epoch: 35 | Loss: 3.649886366474675e-07 \n",
      "Epoch: 36 | Loss: 3.597381237341324e-07 \n",
      "Epoch: 37 | Loss: 3.543530056049349e-07 \n",
      "Epoch: 38 | Loss: 3.4942041793328826e-07 \n",
      "Epoch: 39 | Loss: 3.4428359185767476e-07 \n",
      "Epoch: 40 | Loss: 3.392528924450744e-07 \n",
      "Epoch: 41 | Loss: 3.3442694302721065e-07 \n",
      "Epoch: 42 | Loss: 3.29635668094852e-07 \n",
      "Epoch: 43 | Loss: 3.2497842994416715e-07 \n",
      "Epoch: 44 | Loss: 3.2048598086475977e-07 \n",
      "Epoch: 45 | Loss: 3.1589411264576484e-07 \n",
      "Epoch: 46 | Loss: 3.112056674581254e-07 \n",
      "Epoch: 47 | Loss: 3.0668098816022393e-07 \n",
      "Epoch: 48 | Loss: 3.023172325811174e-07 \n",
      "Epoch: 49 | Loss: 2.9823883096469217e-07 \n",
      "Epoch: 50 | Loss: 2.9377821419984684e-07 \n",
      "Epoch: 51 | Loss: 2.896015871556301e-07 \n",
      "Epoch: 52 | Loss: 2.853615228559647e-07 \n",
      "Epoch: 53 | Loss: 2.812150796671631e-07 \n",
      "Epoch: 54 | Loss: 2.7752639653044753e-07 \n",
      "Epoch: 55 | Loss: 2.7322425921738613e-07 \n",
      "Epoch: 56 | Loss: 2.6940830366584123e-07 \n",
      "Epoch: 57 | Loss: 2.6555863996691187e-07 \n",
      "Epoch: 58 | Loss: 2.6179668566328473e-07 \n",
      "Epoch: 59 | Loss: 2.579727720330993e-07 \n",
      "Epoch: 60 | Loss: 2.5403119252587203e-07 \n",
      "Epoch: 61 | Loss: 2.5058449182324694e-07 \n",
      "Epoch: 62 | Loss: 2.468721618242853e-07 \n",
      "Epoch: 63 | Loss: 2.434463226563821e-07 \n",
      "Epoch: 64 | Loss: 2.3975940166565124e-07 \n",
      "Epoch: 65 | Loss: 2.364113811381685e-07 \n",
      "Epoch: 66 | Loss: 2.3305921104110894e-07 \n",
      "Epoch: 67 | Loss: 2.2967469703871757e-07 \n",
      "Epoch: 68 | Loss: 2.2637073016085196e-07 \n",
      "Epoch: 69 | Loss: 2.2303544255919405e-07 \n",
      "Epoch: 70 | Loss: 2.197796789005224e-07 \n",
      "Epoch: 71 | Loss: 2.1676424921679427e-07 \n",
      "Epoch: 72 | Loss: 2.1355486978791305e-07 \n",
      "Epoch: 73 | Loss: 2.1050237819508766e-07 \n",
      "Epoch: 74 | Loss: 2.077370027109282e-07 \n",
      "Epoch: 75 | Loss: 2.0438551473489497e-07 \n",
      "Epoch: 76 | Loss: 2.0168636183370836e-07 \n",
      "Epoch: 77 | Loss: 1.9851313481922261e-07 \n",
      "Epoch: 78 | Loss: 1.958279085556569e-07 \n",
      "Epoch: 79 | Loss: 1.929570316860918e-07 \n",
      "Epoch: 80 | Loss: 1.90334731087205e-07 \n",
      "Epoch: 81 | Loss: 1.875044404187065e-07 \n",
      "Epoch: 82 | Loss: 1.85019644050044e-07 \n",
      "Epoch: 83 | Loss: 1.8222914377474808e-07 \n",
      "Epoch: 84 | Loss: 1.7946013031178154e-07 \n",
      "Epoch: 85 | Loss: 1.7673619367997162e-07 \n",
      "Epoch: 86 | Loss: 1.7456704881624319e-07 \n",
      "Epoch: 87 | Loss: 1.719770921226882e-07 \n",
      "Epoch: 88 | Loss: 1.692872046987759e-07 \n",
      "Epoch: 89 | Loss: 1.6716444406483788e-07 \n",
      "Epoch: 90 | Loss: 1.6451264173156233e-07 \n",
      "Epoch: 91 | Loss: 1.6223299326156848e-07 \n",
      "Epoch: 92 | Loss: 1.5999216884665657e-07 \n",
      "Epoch: 93 | Loss: 1.5751317050671787e-07 \n",
      "Epoch: 94 | Loss: 1.5528263475061976e-07 \n",
      "Epoch: 95 | Loss: 1.5306801515180268e-07 \n",
      "Epoch: 96 | Loss: 1.5086931171026663e-07 \n",
      "Epoch: 97 | Loss: 1.486865244260116e-07 \n",
      "Epoch: 98 | Loss: 1.465196532990376e-07 \n",
      "Epoch: 99 | Loss: 1.4454536767516402e-07 \n",
      "Epoch: 100 | Loss: 1.4256261238188017e-07 \n",
      "Epoch: 101 | Loss: 1.402670477546053e-07 \n",
      "Epoch: 102 | Loss: 1.383352810080396e-07 \n",
      "Epoch: 103 | Loss: 1.364171566820005e-07 \n",
      "Epoch: 104 | Loss: 1.3432082823783276e-07 \n",
      "Epoch: 105 | Loss: 1.3243067087387317e-07 \n",
      "Epoch: 106 | Loss: 1.3053318070888054e-07 \n",
      "Epoch: 107 | Loss: 1.2864944665125222e-07 \n",
      "Epoch: 108 | Loss: 1.2679976180152153e-07 \n",
      "Epoch: 109 | Loss: 1.2510759006545413e-07 \n",
      "Epoch: 110 | Loss: 1.2326341902735294e-07 \n",
      "Epoch: 111 | Loss: 1.2139258842580603e-07 \n",
      "Epoch: 112 | Loss: 1.1977709846178186e-07 \n",
      "Epoch: 113 | Loss: 1.1797283150372095e-07 \n",
      "Epoch: 114 | Loss: 1.1618232065302436e-07 \n",
      "Epoch: 115 | Loss: 1.1456268111942336e-07 \n",
      "Epoch: 116 | Loss: 1.130913460656302e-07 \n",
      "Epoch: 117 | Loss: 1.1124194543299382e-07 \n",
      "Epoch: 118 | Loss: 1.095991137844976e-07 \n",
      "Epoch: 119 | Loss: 1.0831308827619068e-07 \n",
      "Epoch: 120 | Loss: 1.066359800461214e-07 \n",
      "Epoch: 121 | Loss: 1.0508443892831565e-07 \n",
      "Epoch: 122 | Loss: 1.0354438018111978e-07 \n",
      "Epoch: 123 | Loss: 1.0214580470346846e-07 \n",
      "Epoch: 124 | Loss: 1.006092134048231e-07 \n",
      "Epoch: 125 | Loss: 9.908444553730078e-08 \n",
      "Epoch: 126 | Loss: 9.771633813215885e-08 \n",
      "Epoch: 127 | Loss: 9.641212272981647e-08 \n",
      "Epoch: 128 | Loss: 9.488331897955504e-08 \n",
      "Epoch: 129 | Loss: 9.35806383495219e-08 \n",
      "Epoch: 130 | Loss: 9.225118446920533e-08 \n",
      "Epoch: 131 | Loss: 9.093128028325737e-08 \n",
      "Epoch: 132 | Loss: 8.962092579167802e-08 \n",
      "Epoch: 133 | Loss: 8.832012099446729e-08 \n",
      "Epoch: 134 | Loss: 8.702886589162517e-08 \n",
      "Epoch: 135 | Loss: 8.586636113250279e-08 \n",
      "Epoch: 136 | Loss: 8.450871291643125e-08 \n",
      "Epoch: 137 | Loss: 8.336314749612939e-08 \n",
      "Epoch: 138 | Loss: 8.210872692870907e-08 \n",
      "Epoch: 139 | Loss: 8.08973368293664e-08 \n",
      "Epoch: 140 | Loss: 7.974341542649199e-08 \n",
      "Epoch: 141 | Loss: 7.867970452934969e-08 \n",
      "Epoch: 142 | Loss: 7.74451223151118e-08 \n",
      "Epoch: 143 | Loss: 7.634866960870568e-08 \n",
      "Epoch: 144 | Loss: 7.527592060796451e-08 \n",
      "Epoch: 145 | Loss: 7.419504299832624e-08 \n",
      "Epoch: 146 | Loss: 7.316907613130752e-08 \n",
      "Epoch: 147 | Loss: 7.210343255792395e-08 \n",
      "Epoch: 148 | Loss: 7.115414746294846e-08 \n",
      "Epoch: 149 | Loss: 7.01495537214214e-08 \n",
      "Epoch: 150 | Loss: 6.898409310451825e-08 \n",
      "Epoch: 151 | Loss: 6.794948603783268e-08 \n",
      "Epoch: 152 | Loss: 6.704300403725938e-08 \n",
      "Epoch: 153 | Loss: 6.605324642805499e-08 \n",
      "Epoch: 154 | Loss: 6.515938366646878e-08 \n",
      "Epoch: 155 | Loss: 6.427200105463271e-08 \n",
      "Epoch: 156 | Loss: 6.330270707621821e-08 \n",
      "Epoch: 157 | Loss: 6.241344863155973e-08 \n",
      "Epoch: 158 | Loss: 6.1429545894498e-08 \n",
      "Epoch: 159 | Loss: 6.071104508009739e-08 \n",
      "Epoch: 160 | Loss: 5.974067107672454e-08 \n",
      "Epoch: 161 | Loss: 5.88768784837157e-08 \n",
      "Epoch: 162 | Loss: 5.806151648357627e-08 \n",
      "Epoch: 163 | Loss: 5.7210002069041366e-08 \n",
      "Epoch: 164 | Loss: 5.6406463500024984e-08 \n",
      "Epoch: 165 | Loss: 5.547121872950811e-08 \n",
      "Epoch: 166 | Loss: 5.485640031110961e-08 \n",
      "Epoch: 167 | Loss: 5.392075763666071e-08 \n",
      "Epoch: 168 | Loss: 5.331503416527994e-08 \n",
      "Epoch: 169 | Loss: 5.243236955720931e-08 \n",
      "Epoch: 170 | Loss: 5.162337401998229e-08 \n",
      "Epoch: 171 | Loss: 5.086008059151936e-08 \n",
      "Epoch: 172 | Loss: 5.027163751947228e-08 \n",
      "Epoch: 173 | Loss: 4.947946763422806e-08 \n",
      "Epoch: 174 | Loss: 4.882195980826509e-08 \n",
      "Epoch: 175 | Loss: 4.814347676074249e-08 \n",
      "Epoch: 176 | Loss: 4.736835990115651e-08 \n",
      "Epoch: 177 | Loss: 4.6725062929908745e-08 \n",
      "Epoch: 178 | Loss: 4.608642711900757e-08 \n",
      "Epoch: 179 | Loss: 4.532813591140439e-08 \n",
      "Epoch: 180 | Loss: 4.467420922082965e-08 \n",
      "Epoch: 181 | Loss: 4.414732757140882e-08 \n",
      "Epoch: 182 | Loss: 4.34900471191213e-08 \n",
      "Epoch: 183 | Loss: 4.2789736198756145e-08 \n",
      "Epoch: 184 | Loss: 4.2238070818712004e-08 \n",
      "Epoch: 185 | Loss: 4.1607108869357035e-08 \n",
      "Epoch: 186 | Loss: 4.106351525479113e-08 \n",
      "Epoch: 187 | Loss: 4.0371105569647625e-08 \n",
      "Epoch: 188 | Loss: 3.9870258206065046e-08 \n",
      "Epoch: 189 | Loss: 3.924583324987907e-08 \n",
      "Epoch: 190 | Loss: 3.8637608668068424e-08 \n",
      "Epoch: 191 | Loss: 3.814767524090712e-08 \n",
      "Epoch: 192 | Loss: 3.753689270524774e-08 \n",
      "Epoch: 193 | Loss: 3.7054007862025173e-08 \n",
      "Epoch: 194 | Loss: 3.656333547041868e-08 \n",
      "Epoch: 195 | Loss: 3.596551323425956e-08 \n",
      "Epoch: 196 | Loss: 3.547074811649509e-08 \n",
      "Epoch: 197 | Loss: 3.499053491395898e-08 \n",
      "Epoch: 198 | Loss: 3.443830109972623e-08 \n",
      "Epoch: 199 | Loss: 3.4029199014184996e-08 \n",
      "Epoch: 200 | Loss: 3.355899025336839e-08 \n",
      "Epoch: 201 | Loss: 3.298629280834575e-08 \n",
      "Epoch: 202 | Loss: 3.250215740990825e-08 \n",
      "Epoch: 203 | Loss: 3.210521981600323e-08 \n",
      "Epoch: 204 | Loss: 3.164819872836233e-08 \n",
      "Epoch: 205 | Loss: 3.120499059150461e-08 \n",
      "Epoch: 206 | Loss: 3.073438392675598e-08 \n",
      "Epoch: 207 | Loss: 3.028736728083459e-08 \n",
      "Epoch: 208 | Loss: 2.9823695513186976e-08 \n",
      "Epoch: 209 | Loss: 2.9463308237609453e-08 \n",
      "Epoch: 210 | Loss: 2.8926763206982287e-08 \n",
      "Epoch: 211 | Loss: 2.85718328996154e-08 \n",
      "Epoch: 212 | Loss: 2.818984512487077e-08 \n",
      "Epoch: 213 | Loss: 2.776192786768661e-08 \n",
      "Epoch: 214 | Loss: 2.734691406658385e-08 \n",
      "Epoch: 215 | Loss: 2.6896941562881693e-08 \n",
      "Epoch: 216 | Loss: 2.6630459615262225e-08 \n",
      "Epoch: 217 | Loss: 2.6186398827121593e-08 \n",
      "Epoch: 218 | Loss: 2.5783322143979603e-08 \n",
      "Epoch: 219 | Loss: 2.5494784949842142e-08 \n",
      "Epoch: 220 | Loss: 2.5087786070798757e-08 \n",
      "Epoch: 221 | Loss: 2.472989990565111e-08 \n",
      "Epoch: 222 | Loss: 2.4329267489520134e-08 \n",
      "Epoch: 223 | Loss: 2.404891574769863e-08 \n",
      "Epoch: 224 | Loss: 2.3653740299778292e-08 \n",
      "Epoch: 225 | Loss: 2.333291604372789e-08 \n",
      "Epoch: 226 | Loss: 2.2987762804405065e-08 \n",
      "Epoch: 227 | Loss: 2.267148602186353e-08 \n",
      "Epoch: 228 | Loss: 2.2331335003400454e-08 \n",
      "Epoch: 229 | Loss: 2.2019605694367783e-08 \n",
      "Epoch: 230 | Loss: 2.168445689676446e-08 \n",
      "Epoch: 231 | Loss: 2.1368805391830392e-08 \n",
      "Epoch: 232 | Loss: 2.1131427274667658e-08 \n",
      "Epoch: 233 | Loss: 2.0736024453071877e-08 \n",
      "Epoch: 234 | Loss: 2.0493928332143696e-08 \n",
      "Epoch: 235 | Loss: 2.0195386696286732e-08 \n",
      "Epoch: 236 | Loss: 1.987433506656089e-08 \n",
      "Epoch: 237 | Loss: 1.957255335582886e-08 \n",
      "Epoch: 238 | Loss: 1.93208506971132e-08 \n",
      "Epoch: 239 | Loss: 1.903094926092308e-08 \n",
      "Epoch: 240 | Loss: 1.8799084955389844e-08 \n",
      "Epoch: 241 | Loss: 1.8489402009436162e-08 \n",
      "Epoch: 242 | Loss: 1.8198306861449964e-08 \n",
      "Epoch: 243 | Loss: 1.797911863832269e-08 \n",
      "Epoch: 244 | Loss: 1.7730428680806654e-08 \n",
      "Epoch: 245 | Loss: 1.7491458947915817e-08 \n",
      "Epoch: 246 | Loss: 1.7269030649913475e-08 \n",
      "Epoch: 247 | Loss: 1.698737150945817e-08 \n",
      "Epoch: 248 | Loss: 1.671565996730351e-08 \n",
      "Epoch: 249 | Loss: 1.653575054660905e-08 \n",
      "Epoch: 250 | Loss: 1.626767698326148e-08 \n",
      "Epoch: 251 | Loss: 1.5994430668797577e-08 \n",
      "Epoch: 252 | Loss: 1.5759951565996744e-08 \n",
      "Epoch: 253 | Loss: 1.5498244465561584e-08 \n",
      "Epoch: 254 | Loss: 1.53250425682927e-08 \n",
      "Epoch: 255 | Loss: 1.5116938811843283e-08 \n",
      "Epoch: 256 | Loss: 1.4910312984284246e-08 \n",
      "Epoch: 257 | Loss: 1.4740635378984734e-08 \n",
      "Epoch: 258 | Loss: 1.4480349364021095e-08 \n",
      "Epoch: 259 | Loss: 1.42781573231332e-08 \n",
      "Epoch: 260 | Loss: 1.4077443211135687e-08 \n",
      "Epoch: 261 | Loss: 1.3857516023563221e-08 \n",
      "Epoch: 262 | Loss: 1.365975776934647e-08 \n",
      "Epoch: 263 | Loss: 1.34634774440201e-08 \n",
      "Epoch: 264 | Loss: 1.3254975783638656e-08 \n",
      "Epoch: 265 | Loss: 1.3055114322924055e-08 \n",
      "Epoch: 266 | Loss: 1.2849795893998817e-08 \n",
      "Epoch: 267 | Loss: 1.2757709555444308e-08 \n",
      "Epoch: 268 | Loss: 1.251584080819157e-08 \n",
      "Epoch: 269 | Loss: 1.2379587133182213e-08 \n",
      "Epoch: 270 | Loss: 1.214158373841201e-08 \n",
      "Epoch: 271 | Loss: 1.1988049664068967e-08 \n",
      "Epoch: 272 | Loss: 1.1804161204054253e-08 \n",
      "Epoch: 273 | Loss: 1.1653128240141086e-08 \n",
      "Epoch: 274 | Loss: 1.146554495790042e-08 \n",
      "Epoch: 275 | Loss: 1.1335089311614865e-08 \n",
      "Epoch: 276 | Loss: 1.1137899491586722e-08 \n",
      "Epoch: 277 | Loss: 1.1009490208380157e-08 \n",
      "Epoch: 278 | Loss: 1.0875567113544093e-08 \n",
      "Epoch: 279 | Loss: 1.0682470019673929e-08 \n",
      "Epoch: 280 | Loss: 1.0556789220572682e-08 \n",
      "Epoch: 281 | Loss: 1.0378073511674302e-08 \n",
      "Epoch: 282 | Loss: 1.025409801513888e-08 \n",
      "Epoch: 283 | Loss: 1.0124949767487124e-08 \n",
      "Epoch: 284 | Loss: 9.938560197042534e-09 \n",
      "Epoch: 285 | Loss: 9.81731318461243e-09 \n",
      "Epoch: 286 | Loss: 9.690893421065994e-09 \n",
      "Epoch: 287 | Loss: 9.508596576779382e-09 \n",
      "Epoch: 288 | Loss: 9.384223176311934e-09 \n",
      "Epoch: 289 | Loss: 9.26638676901348e-09 \n",
      "Epoch: 290 | Loss: 9.143604984274134e-09 \n",
      "Epoch: 291 | Loss: 9.021732694236562e-09 \n",
      "Epoch: 292 | Loss: 8.845802312862361e-09 \n",
      "Epoch: 293 | Loss: 8.725976385903778e-09 \n",
      "Epoch: 294 | Loss: 8.607059953646967e-09 \n",
      "Epoch: 295 | Loss: 8.47785486257635e-09 \n",
      "Epoch: 296 | Loss: 8.365816484001698e-09 \n",
      "Epoch: 297 | Loss: 8.207109658542322e-09 \n",
      "Epoch: 298 | Loss: 8.133440587698715e-09 \n",
      "Epoch: 299 | Loss: 7.966491466504522e-09 \n",
      "Epoch: 300 | Loss: 7.852804628782906e-09 \n",
      "Epoch: 301 | Loss: 7.780727173667401e-09 \n",
      "Epoch: 302 | Loss: 7.668404577998444e-09 \n",
      "Epoch: 303 | Loss: 7.546589131379733e-09 \n",
      "Epoch: 304 | Loss: 7.435858151438879e-09 \n",
      "Epoch: 305 | Loss: 7.365827059402363e-09 \n",
      "Epoch: 306 | Loss: 7.21712467566249e-09 \n",
      "Epoch: 307 | Loss: 7.098833521013148e-09 \n",
      "Epoch: 308 | Loss: 7.030394044704735e-09 \n",
      "Epoch: 309 | Loss: 6.923528417246416e-09 \n",
      "Epoch: 310 | Loss: 6.855998435639776e-09 \n",
      "Epoch: 311 | Loss: 6.702805421809899e-09 \n",
      "Epoch: 312 | Loss: 6.636412308580475e-09 \n",
      "Epoch: 313 | Loss: 6.532502538902918e-09 \n",
      "Epoch: 314 | Loss: 6.429502263927134e-09 \n",
      "Epoch: 315 | Loss: 6.350603598548332e-09 \n",
      "Epoch: 316 | Loss: 6.2489675656252075e-09 \n",
      "Epoch: 317 | Loss: 6.184848189150216e-09 \n",
      "Epoch: 318 | Loss: 6.10776851317496e-09 \n",
      "Epoch: 319 | Loss: 5.985214102111058e-09 \n",
      "Epoch: 320 | Loss: 5.9090439208375756e-09 \n",
      "Epoch: 321 | Loss: 5.8110458667215426e-09 \n",
      "Epoch: 322 | Loss: 5.7492002270009834e-09 \n",
      "Epoch: 323 | Loss: 5.65256641493761e-09 \n",
      "Epoch: 324 | Loss: 5.578669970418559e-09 \n",
      "Epoch: 325 | Loss: 5.51818857275066e-09 \n",
      "Epoch: 326 | Loss: 5.423373750090832e-09 \n",
      "Epoch: 327 | Loss: 5.35919753019698e-09 \n",
      "Epoch: 328 | Loss: 5.299853000906296e-09 \n",
      "Epoch: 329 | Loss: 5.19457898917608e-09 \n",
      "Epoch: 330 | Loss: 5.102720024297014e-09 \n",
      "Epoch: 331 | Loss: 5.04473973705899e-09 \n",
      "Epoch: 332 | Loss: 4.982950940757291e-09 \n",
      "Epoch: 333 | Loss: 4.925652774545597e-09 \n",
      "Epoch: 334 | Loss: 4.836294920096407e-09 \n",
      "Epoch: 335 | Loss: 4.7478465603489894e-09 \n",
      "Epoch: 336 | Loss: 4.719822754850611e-09 \n",
      "Epoch: 337 | Loss: 4.620460458681919e-09 \n",
      "Epoch: 338 | Loss: 4.5340584620134905e-09 \n",
      "Epoch: 339 | Loss: 4.510411599767394e-09 \n",
      "Epoch: 340 | Loss: 4.421337962412508e-09 \n",
      "Epoch: 341 | Loss: 4.359435479273088e-09 \n",
      "Epoch: 342 | Loss: 4.301966782804811e-09 \n",
      "Epoch: 343 | Loss: 4.218520643917145e-09 \n",
      "Epoch: 344 | Loss: 4.162302502663806e-09 \n",
      "Epoch: 345 | Loss: 4.139565135119483e-09 \n",
      "Epoch: 346 | Loss: 4.028379407827742e-09 \n",
      "Epoch: 347 | Loss: 3.994955477537587e-09 \n",
      "Epoch: 348 | Loss: 3.939987891499186e-09 \n",
      "Epoch: 349 | Loss: 3.885588739649393e-09 \n",
      "Epoch: 350 | Loss: 3.834884410025552e-09 \n",
      "Epoch: 351 | Loss: 3.802824721788056e-09 \n",
      "Epoch: 352 | Loss: 3.731258857442299e-09 \n",
      "Epoch: 353 | Loss: 3.6710048334498424e-09 \n",
      "Epoch: 354 | Loss: 3.6218921195541043e-09 \n",
      "Epoch: 355 | Loss: 3.542027116054669e-09 \n",
      "Epoch: 356 | Loss: 3.5211087379138917e-09 \n",
      "Epoch: 357 | Loss: 3.469551757007139e-09 \n",
      "Epoch: 358 | Loss: 3.4115714697691146e-09 \n",
      "Epoch: 359 | Loss: 3.360810296726413e-09 \n",
      "Epoch: 360 | Loss: 3.313743945909664e-09 \n",
      "Epoch: 361 | Loss: 3.2671323424438015e-09 \n",
      "Epoch: 362 | Loss: 3.2342200029233936e-09 \n",
      "Epoch: 363 | Loss: 3.194372766301967e-09 \n",
      "Epoch: 364 | Loss: 3.112745616817847e-09 \n",
      "Epoch: 365 | Loss: 3.0900650926923845e-09 \n",
      "Epoch: 366 | Loss: 3.0450451049546245e-09 \n",
      "Epoch: 367 | Loss: 3.022705641342327e-09 \n",
      "Epoch: 368 | Loss: 2.943352228612639e-09 \n",
      "Epoch: 369 | Loss: 2.9213538255135063e-09 \n",
      "Epoch: 370 | Loss: 2.8742306312778965e-09 \n",
      "Epoch: 371 | Loss: 2.830802259268239e-09 \n",
      "Epoch: 372 | Loss: 2.809258603519993e-09 \n",
      "Epoch: 373 | Loss: 2.741728621913353e-09 \n",
      "Epoch: 374 | Loss: 2.6961970434058458e-09 \n",
      "Epoch: 375 | Loss: 2.6905695449386258e-09 \n",
      "Epoch: 376 | Loss: 2.654132913448848e-09 \n",
      "Epoch: 377 | Loss: 2.609397142805392e-09 \n",
      "Epoch: 378 | Loss: 2.5591475605324376e-09 \n",
      "Epoch: 379 | Loss: 2.515207597753033e-09 \n",
      "Epoch: 380 | Loss: 2.494800810382003e-09 \n",
      "Epoch: 381 | Loss: 2.4543282961531077e-09 \n",
      "Epoch: 382 | Loss: 2.4342625692952424e-09 \n",
      "Epoch: 383 | Loss: 2.3913457880553324e-09 \n",
      "Epoch: 384 | Loss: 2.3433699425368104e-09 \n",
      "Epoch: 385 | Loss: 2.3012489691609517e-09 \n",
      "Epoch: 386 | Loss: 2.2817516764916945e-09 \n",
      "Epoch: 387 | Loss: 2.2403128241421655e-09 \n",
      "Epoch: 388 | Loss: 2.2210429051483516e-09 \n",
      "Epoch: 389 | Loss: 2.1802861738251522e-09 \n",
      "Epoch: 390 | Loss: 2.1639152691932395e-09 \n",
      "Epoch: 391 | Loss: 2.123499598383205e-09 \n",
      "Epoch: 392 | Loss: 2.104798113577999e-09 \n",
      "Epoch: 393 | Loss: 2.0650645637942944e-09 \n",
      "Epoch: 394 | Loss: 2.0465904526645318e-09 \n",
      "Epoch: 395 | Loss: 2.0075390239071567e-09 \n",
      "Epoch: 396 | Loss: 1.9838921616610605e-09 \n",
      "Epoch: 397 | Loss: 1.9451817934168503e-09 \n",
      "Epoch: 398 | Loss: 1.907039859361248e-09 \n",
      "Epoch: 399 | Loss: 1.889247869257815e-09 \n",
      "Epoch: 400 | Loss: 1.871569565992104e-09 \n",
      "Epoch: 401 | Loss: 1.834223439800553e-09 \n",
      "Epoch: 402 | Loss: 1.7919319361681119e-09 \n",
      "Epoch: 403 | Loss: 1.774708380253287e-09 \n",
      "Epoch: 404 | Loss: 1.738271748763509e-09 \n",
      "Epoch: 405 | Loss: 1.7212755665241275e-09 \n",
      "Epoch: 406 | Loss: 1.7099068827519659e-09 \n",
      "Epoch: 407 | Loss: 1.6687522474967409e-09 \n",
      "Epoch: 408 | Loss: 1.6709691408323124e-09 \n",
      "Epoch: 409 | Loss: 1.635555690882029e-09 \n",
      "Epoch: 410 | Loss: 1.6146373127412517e-09 \n",
      "Epoch: 411 | Loss: 1.5957652976794634e-09 \n",
      "Epoch: 412 | Loss: 1.5611476555932313e-09 \n",
      "Epoch: 413 | Loss: 1.5450609680556227e-09 \n",
      "Epoch: 414 | Loss: 1.5181740309344605e-09 \n",
      "Epoch: 415 | Loss: 1.4952661331335548e-09 \n",
      "Epoch: 416 | Loss: 1.4638885659223888e-09 \n",
      "Epoch: 417 | Loss: 1.4417764759855345e-09 \n",
      "Epoch: 418 | Loss: 1.4263719094742555e-09 \n",
      "Epoch: 419 | Loss: 1.4154579730529804e-09 \n",
      "Epoch: 420 | Loss: 1.3959038369648624e-09 \n",
      "Epoch: 421 | Loss: 1.3783960639557336e-09 \n",
      "Epoch: 422 | Loss: 1.3362750905798748e-09 \n",
      "Epoch: 423 | Loss: 1.3313297131389845e-09 \n",
      "Epoch: 424 | Loss: 1.3164935808163136e-09 \n",
      "Epoch: 425 | Loss: 1.2810232874471694e-09 \n",
      "Epoch: 426 | Loss: 1.2871623766841367e-09 \n",
      "Epoch: 427 | Loss: 1.2560690265672747e-09 \n",
      "Epoch: 428 | Loss: 1.235548552358523e-09 \n",
      "Epoch: 429 | Loss: 1.211276412504958e-09 \n",
      "Epoch: 430 | Loss: 1.1909833119716495e-09 \n",
      "Epoch: 431 | Loss: 1.1930865184694994e-09 \n",
      "Epoch: 432 | Loss: 1.17694298751303e-09 \n",
      "Epoch: 433 | Loss: 1.1492033991089556e-09 \n",
      "Epoch: 434 | Loss: 1.1198153515579179e-09 \n",
      "Epoch: 435 | Loss: 1.116234216169687e-09 \n",
      "Epoch: 436 | Loss: 1.1062297744501848e-09 \n",
      "Epoch: 437 | Loss: 1.1027623258996755e-09 \n",
      "Epoch: 438 | Loss: 1.0641656444931868e-09 \n",
      "Epoch: 439 | Loss: 1.0452367860125378e-09 \n",
      "Epoch: 440 | Loss: 1.0321059562556911e-09 \n",
      "Epoch: 441 | Loss: 1.0343228495912626e-09 \n",
      "Epoch: 442 | Loss: 1.004309524432756e-09 \n",
      "Epoch: 443 | Loss: 1.0061853572551627e-09 \n",
      "Epoch: 444 | Loss: 9.933955880114809e-10 \n",
      "Epoch: 445 | Loss: 9.642917575547472e-10 \n",
      "Epoch: 446 | Loss: 9.51615675148787e-10 \n",
      "Epoch: 447 | Loss: 9.53377821133472e-10 \n",
      "Epoch: 448 | Loss: 9.356995178677607e-10 \n",
      "Epoch: 449 | Loss: 9.266045708500315e-10 \n",
      "Epoch: 450 | Loss: 9.142695489572361e-10 \n",
      "Epoch: 451 | Loss: 8.96818619366968e-10 \n",
      "Epoch: 452 | Loss: 8.708411769475788e-10 \n",
      "Epoch: 453 | Loss: 8.588472155679483e-10 \n",
      "Epoch: 454 | Loss: 8.608367352280766e-10 \n",
      "Epoch: 455 | Loss: 8.335518941748887e-10 \n",
      "Epoch: 456 | Loss: 8.352003533218522e-10 \n",
      "Epoch: 457 | Loss: 8.235474524553865e-10 \n",
      "Epoch: 458 | Loss: 7.985931915754918e-10 \n",
      "Epoch: 459 | Loss: 7.985931915754918e-10 \n",
      "Epoch: 460 | Loss: 7.823928172001615e-10 \n",
      "Epoch: 461 | Loss: 7.710809768468607e-10 \n",
      "Epoch: 462 | Loss: 7.598828233312815e-10 \n",
      "Epoch: 463 | Loss: 7.469225238310173e-10 \n",
      "Epoch: 464 | Loss: 7.48798356653424e-10 \n",
      "Epoch: 465 | Loss: 7.37827576813288e-10 \n",
      "Epoch: 466 | Loss: 7.12361725163646e-10 \n",
      "Epoch: 467 | Loss: 7.015046321612317e-10 \n",
      "Epoch: 468 | Loss: 7.015046321612317e-10 \n",
      "Epoch: 469 | Loss: 6.90761225996539e-10 \n",
      "Epoch: 470 | Loss: 6.801315066695679e-10 \n",
      "Epoch: 471 | Loss: 6.801315066695679e-10 \n",
      "Epoch: 472 | Loss: 6.696154741803184e-10 \n",
      "Epoch: 473 | Loss: 6.471623237302992e-10 \n",
      "Epoch: 474 | Loss: 6.471623237302992e-10 \n",
      "Epoch: 475 | Loss: 6.325535650830716e-10 \n",
      "Epoch: 476 | Loss: 6.22378593106987e-10 \n",
      "Epoch: 477 | Loss: 6.22378593106987e-10 \n",
      "Epoch: 478 | Loss: 6.007212505210191e-10 \n",
      "Epoch: 479 | Loss: 6.023697096679825e-10 \n",
      "Epoch: 480 | Loss: 5.907736522203777e-10 \n",
      "Epoch: 481 | Loss: 5.809397407574579e-10 \n",
      "Epoch: 482 | Loss: 5.697984306607395e-10 \n",
      "Epoch: 483 | Loss: 5.600782060355414e-10 \n",
      "Epoch: 484 | Loss: 5.616129783447832e-10 \n",
      "Epoch: 485 | Loss: 5.504716682480648e-10 \n",
      "Epoch: 486 | Loss: 5.409788172983099e-10 \n",
      "Epoch: 487 | Loss: 5.409788172983099e-10 \n",
      "Epoch: 488 | Loss: 5.315996531862766e-10 \n",
      "Epoch: 489 | Loss: 5.209130904404446e-10 \n",
      "Epoch: 490 | Loss: 5.116476131661329e-10 \n",
      "Epoch: 491 | Loss: 5.014157977711875e-10 \n",
      "Epoch: 492 | Loss: 5.024958227295429e-10 \n",
      "Epoch: 493 | Loss: 4.832259037357289e-10 \n",
      "Epoch: 494 | Loss: 4.832259037357289e-10 \n",
      "Epoch: 495 | Loss: 4.743014869745821e-10 \n",
      "Epoch: 496 | Loss: 4.743014869745821e-10 \n",
      "Epoch: 497 | Loss: 4.5571368900709786e-10 \n",
      "Epoch: 498 | Loss: 4.5571368900709786e-10 \n",
      "Epoch: 499 | Loss: 4.376943252282217e-10 \n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(500):\n",
    "    # 1) Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x_data)\n",
    "    \n",
    "    # 2) Compute and print loss\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    print(f\"Epoch: {epoch} | Loss: {loss.item()} \")\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction (after training) 4 7.999975681304932\n",
      "tensor(8.0000)\n"
     ]
    }
   ],
   "source": [
    "# After training\n",
    "hour_var = tensor([[4.0]])\n",
    "y_pred = model(hour_var)\n",
    "print(\"Prediction (after training)\", 4, model(hour_var).data[0][0].item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
