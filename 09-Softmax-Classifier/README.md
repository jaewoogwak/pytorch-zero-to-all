# Softmax Classifier

Pytorchì—ì„œëŠ” `nn.CrossEntropyLoss`ë¥¼ ì œê³µí•œë‹¤.

ì—¬ê¸°ì—ëŠ” Softmaxê°€ ë‚´ë¶€ì ìœ¼ë¡œ êµ¬í˜„ë˜ì–´ ìˆë‹¤.

ë”°ë¼ì„œ `nn.CrossEntropyLoss`ë¥¼ í˜¸ì¶œí•  ë•Œ y_predì™€ y(label)ì„ ê·¸ëŒ€ë¡œ ë„£ê¸°ë§Œ í•˜ë©´ ëœë‹¤. ë§¤ìš° ê°„ë‹¨í•˜ë‹¤.

```python
# Cross Entropy Example
# One hot
# 0: 1 0 0
# 1: 0 1 0
# 2: 0 0 1

Y = np.array([1,0,0])
Y_pred1 = np.array([0.7, 0.2, 0.1])
Y_pred2 = np.array([0.1, 0.3, 0.6])
print(f"Loss 1: {np.sum(-Y * np.log(Y_pred1)):.4f}")
print(f"Loss 2: {np.sum(-Y * np.log(Y_pred2)):.4f}")

# Expected
# Loss 1: 0.3567
# Loss 2: 2.3026

```

ë¶„ë¥˜í•  ê²ƒì´ ì—¬ëŸ¬ ê°œë”ë¼ë„ ê·¸ëŒ€ë¡œ í•˜ë©´ëœë‹¤. (Batch loss)

```python
# target is of size nBatch
# each element in target has to have 0 <= value < nClasses (0-2)
# Input is class, not one-hot
Y = tensor([2, 0, 1], requires_grad=False)

# input is of size nBatch x nClasses = 2 x 4
# Y_pred are logits (not softmax)
Y_pred1 = tensor([[0.1, 0.2, 0.9],
                  [1.1, 0.1, 0.2],
                  [0.2, 2.1, 0.1]])
Y_pred2 = tensor([[0.8, 0.2, 0.3],
                  [0.2, 0.3, 0.5],
                  [0.2, 0.2, 0.5]])

                  l1 = loss(Y_pred1, Y)
l2 = loss(Y_pred2, Y)

print(f"Batch Loss1: {l1.item():.4f} \nBatch Loss2: {l2.data:.4f}")

# Expected
# Batch Loss1: 0.4966
# Batch Loss2: 1.2389
```

[ì‹¤ìŠµ ì½”ë“œ](https://github.com/jaewoogwak/pytorch-zero-to-all/blob/master/09-Softmax-Classifier/softmax_mnist.ipynb)

### ğŸš€ Exercise

ìì—°ì–´ì²˜ë¦¬ë°ì‹¤ìŠµ ìˆ˜ì—… ì‹œê°„ì— ë°°ìš´ [Pytorch Lightning](https://lightning.ai/docs/pytorch/stable/)ì„ ì‹¤ìŠµ ì½”ë“œì— ì ìš©ì‹œì¼œë³´ì•˜ë‹¤.

[PL ì½”ë“œ](https://github.com/jaewoogwak/pytorch-zero-to-all/blob/master/09-Softmax-Classifier/sofmax_mnist_with_pl.ipynb)

### Reference

-   https://baeseongsu.github.io/posts/pytorch-lightning-introduction/

-   https://lightning.ai/docs/pytorch/stable/starter/introduction.html

-   https://github.com/jaewoogwak/nlp/blob/master/ch02/with_pl.ipynb
